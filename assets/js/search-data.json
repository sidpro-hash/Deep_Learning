{
  
    
        "post0": {
            "title": "YOLO V3",
            "content": "import cv2 import numpy as np import time . # We don&#39;t need to write our owan loaing function # This function returns model objects that we can use later on for prediction net = cv2.dnn.readNet(&#39;/content/drive/My Drive/ObjectDetectionv2/yolov3.weights&#39;,&#39;/content/drive/My Drive/ObjectDetectionv2/yolov3.cfg&#39;) # Extract the objects name from coco.name and put everything into list classes = [] with open(&#39;/content/drive/My Drive/ObjectDetectionv2/coco.names&#39;,&#39;r&#39;) as f: classes = f.read().splitlines() print(classes) . [&#39;person&#39;, &#39;bicycle&#39;, &#39;car&#39;, &#39;motorbike&#39;, &#39;aeroplane&#39;, &#39;bus&#39;, &#39;train&#39;, &#39;truck&#39;, &#39;boat&#39;, &#39;traffic light&#39;, &#39;fire hydrant&#39;, &#39;stop sign&#39;, &#39;parking meter&#39;, &#39;bench&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;horse&#39;, &#39;sheep&#39;, &#39;cow&#39;, &#39;elephant&#39;, &#39;bear&#39;, &#39;zebra&#39;, &#39;giraffe&#39;, &#39;backpack&#39;, &#39;umbrella&#39;, &#39;handbag&#39;, &#39;tie&#39;, &#39;suitcase&#39;, &#39;frisbee&#39;, &#39;skis&#39;, &#39;snowboard&#39;, &#39;sports ball&#39;, &#39;kite&#39;, &#39;baseball bat&#39;, &#39;baseball glove&#39;, &#39;skateboard&#39;, &#39;surfboard&#39;, &#39;tennis racket&#39;, &#39;bottle&#39;, &#39;wine glass&#39;, &#39;cup&#39;, &#39;fork&#39;, &#39;knife&#39;, &#39;spoon&#39;, &#39;bowl&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;sandwich&#39;, &#39;orange&#39;, &#39;broccoli&#39;, &#39;carrot&#39;, &#39;hot dog&#39;, &#39;pizza&#39;, &#39;donut&#39;, &#39;cake&#39;, &#39;chair&#39;, &#39;sofa&#39;, &#39;pottedplant&#39;, &#39;bed&#39;, &#39;diningtable&#39;, &#39;toilet&#39;, &#39;tvmonitor&#39;, &#39;laptop&#39;, &#39;mouse&#39;, &#39;remote&#39;, &#39;keyboard&#39;, &#39;cell phone&#39;, &#39;microwave&#39;, &#39;oven&#39;, &#39;toaster&#39;, &#39;sink&#39;, &#39;refrigerator&#39;, &#39;book&#39;, &#39;clock&#39;, &#39;vase&#39;, &#39;scissors&#39;, &#39;teddy bear&#39;, &#39;hair drier&#39;, &#39;toothbrush&#39;] . from google.colab.patches import cv2_imshow cap = cv2.VideoCapture(&#39;/content/drive/My Drive/ObjectDetectionv2/Road_traffic_video2.mp4&#39;) # from video #cap = cv2.VideoCapture(0) # from webcam #img = cv2.imread(&#39;/content/drive/My Drive/ObjectDetectionv2/restaruant2.jpg&#39;) # from image font = cv2.FONT_HERSHEY_PLAIN colors = np.random.uniform(0, 255, size=(len(classes), 3)) #img = cv2.resize(img, None,fx=0.4, fy=0.4) start_time = time.time() img_id = 0 while True: _, img = cap.read() img_id +=1 height, width, _ = img.shape # we need to resize the image in a square 416x416 that can be fit into the Yolo3 # and also we normalize it by dividing the pixel value by 255 # here we use dimension 32xX here X=13 so the input size w,h=(416,416) and it&#39;s work better the bigger X is. blob = cv2.dnn.blobFromImage(img,1/255,(320,320),(0,0,0),swapRB=True,crop=False) # first image,second normalization,third dimension,fourth no any means of substraction # fifth swapRb = true that convert BGR to RGB,sixth no croping # passing this blob into our model inside a net net.setInput(blob) # get output layersname [&#39;yolo_82&#39;, &#39;yolo_94&#39;, &#39;yolo_106&#39;] output_layers_names = net.getUnconnectedOutLayersNames() output_all = net.getLayerNames() # to get names of all layers # print(output_layers_names) # print(output_all) # forward propogation # passing output layers names to get output at that layers layersOutputs = net.forward(output_layers_names) # we need to extract the bounding boxes # confidences and the predicted classes boxes = [] confidences = [] class_ids = [] #print(layersOutputs) #3 boxes with box co-ordinates,confidence score,class score for output in layersOutputs: # 4 box co-ordinates + 1 confidence score + 80 class score = 85 for detection in output: scores = detection[5:] # store 80 class predeictions class_id = np.argmax(scores) #extract highest scores indexes confidence = scores[class_id] if confidence &gt; 0.5: # Object detected # we have normalized img by scalefactor 1/255 # so co-ordinates are appropriate to that img # to get original, denormalize by multiplying it&#39;s original width,height center_x = int(detection[0] * width) center_y = int(detection[1] * height) w = int(detection[2] * width) h = int(detection[3] * height) # Rectangle coordinates # extract the upper left corners positions in order to present them with use of opencv x = int(center_x - w / 2) y = int(center_y - h / 2) # append everything to draw boxes boxes.append([x, y, w, h]) confidences.append(float(confidence)) class_ids.append(class_id) #print(len(boxes)) #third parameter is set under the confidence,last is NMS indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4) #print(indexes) if len(indexes)&gt;0: for i in indexes.flatten(): x, y, w, h = boxes[i] label = str(classes[class_ids[i]]) confidence = str(round(confidences[i],2)) color = colors[class_ids[i]] cv2.rectangle(img, (x, y), (x + w, y + h), color, 6) cv2.putText(img, label + &quot; &quot; + confidence, (x, y + 50), font, 5, color, 3) # cv2.imshow(&#39;Image&#39;,img) DisabledFunctionError: cv2.imshow() is disabled in Colab, # because it causes Jupyter sessions to crash; elapsed_time = time.time() - start_time fps = img_id/elapsed_time cv2.putText(img,&quot;FPS: &quot;+str(round(fps,2)),(10,30),font,3,(0,0,0),1) cv2_imshow(img) key = cv2.waitKey(1) if key==27: break cv2.release() cv2.destroyAllWindows() . Feature Extractor . The previous YOLO versions have used Darknet-19 (a custom neural network architecture written in C and CUDA) as a feature extractor which was of 19 layers as the name suggests. YOLO v2 added 11 more layers to Darknet-19 making it a total 30-layer architecture. Still, the algorithm faced a challenge while detecting small objects due to downsampling the input image and losing fine-grained features. . YOLO V3 came up with a better architecture where the feature extractor used was a hybrid of YOLO v2, Darknet-53 (a network trained on the ImageNet), and Residual networks(ResNet). The network uses 53 convolution layers (hence the name Darknet-53) where the network is built with consecutive 3x3 and 1x1 convolution layers followed by a skip connection (introduced by ResNet to help the activations propagate through deeper layers without gradient diminishing). . The 53 layers of the darknet are further stacked with 53 more layers for the detection head, making YOLO v3 a total of a 106 layer fully convolutional underlying architecture. thus leading to a large architecture, though making it a bit slower as compared to YOLO v2, but enhancing the accuracy at the same time. . For visualizing how the multi-scale extractor would look like, I’m taking an example of a 416x416 image. A stride of a layer is defined as the ratio by which it downsamples the input, and hence the three scales in our case would be 52x52, 26x26, and 13x13 where 13x13 would be used for larger objects and 26x26 and 52x52 would be used for medium and smaller objects. . Multi-scale Detector . An important feature of the YOLO v3 model is its multi-scale detector, which means that the detection for an eventual output of a fully convolutional network is done by applying 1x1 detection kernels on feature maps of three different sizes at three different places. The shape of the kernel is 1x1x(B*(5+C)). . Complete Network Architecture . diagram that beautifully explains the complete architecture of YOLO v3 (Combining both, the extractor and the detector). . . YOLO v3 makes prediction at three scales, which are precisely given by downsampling the dimensions of the input image by 32, 16 and 8 respectively. . The first detection is made by the 82nd layer. For the first 81 layers, the image is down sampled by the network, such that the 81st layer has a stride of 32. If we have an image of 416 x 416, the resultant feature map would be of size 13 x 13. One detection is made here using the 1 x 1 detection kernel, giving us a detection feature map of 13 x 13 x 255. . Then, the feature map from layer 79 is subjected to a few convolutional layers before being up sampled by 2x to dimensions of 26 x 26. This feature map is then depth concatenated with the feature map from layer 61. Then the combined feature maps is again subjected a few 1 x 1 convolutional layers to fuse the features from the earlier layer (61). Then, the second detection is made by the 94th layer, yielding a detection feature map of 26 x 26 x 255. . A similar procedure is followed again, where the feature map from layer 91 is subjected to few convolutional layers before being depth concatenated with a feature map from layer 36. Like before, a few 1 x 1 convolutional layers follow to fuse the information from the previous layer (36). We make the final of the 3 at 106th layer, yielding feature map of size 52 x 52 x 255. . The multi-scale detector is used to ensure that the small objects are also being detected unlike in YOLO v2, where there was constant criticism regarding the same. Upsampled layers concatenated with the previous layers end up preserving the fine-grained features which help in detecting small objects. . The details of how this kernel looks in our model is described below . Working of YOLO v3 . The YOLO v3 network aims to predict bounding boxes (region of interest of the candidate object) of each object along with the probability of the class which the object belongs to. . For this, the model divides every input image into an SxS grid of cells and each grid predicts B bounding boxes and C class probabilities of the objects whose centers fall inside the grid cells. The paper states that each bounding box may specialize in detecting a certain kind of object. . Bounding boxes &quot;B&quot; is associated with the number of anchors being used. Each bounding box has 5+C attributes, where &#39;5&#39; refers to the five bounding box attributes (eg: center coordinates(bx, by), height(bh), width(bw), and confidence score) and C is the number of classes. . Our output from passing this image into a forward pass convolution network is a 3-D tensor because we are working on an SxS image. The output looks like [S, S, B*(5+C)]. . Let’s just understand this better using an example. . . In the above example, we see that our input image is divided into 13 x 13 grid cells. Now, let us understand what happens with taking just a single grid cell. . Due to the multi-scale detection feature of YOLO v3, a detection kernel of three different sizes is applied at three different places, hence the 3 boxes(i.e B=3). YOLO v3 was trained on the COCO dataset with 80 object categories or classes, hence C=80. . Thus, the output is a 3-D tensor as mentioned earlier with dimensions (13, 13, 3*(80+5)). . Anchor Boxes . In the earlier years for detecting an object, scientists used the concept of the sliding window and ran an image classification algorithm on each window. Soon they realized this didn’t make sense and was very inefficient so they moved on to using ConvNets and running the entire image in a single shot. Since the ConvNet outputs square matrices of feature values (i.e something like a 13x13 or 26x26 in case of YOLO) the concept of &quot;grid&quot; came into the picture. We define the square feature matrix as a grid but the real problem came when the objects to detect were not in square shapes. These objects could be in any shape (mostly rectangular). Thus, anchor boxes were started being used. . Anchor boxes are pre-defined boxes that have an aspect ratio set. These aspect ratios are defined beforehand even before training by running a K-means clustering on the entire dataset. These anchor boxes anchor to the grid cells and share the same centroid. YOLO v3 uses 3 anchor boxes for every detection scale, which makes it a total of 9 anchor boxes. . Non-Maximum Suppression . There is a chance that after the single forward pass, the output predicted would have multiple bounding boxes for the same object since the centroid would be the same, but we only need one bounding box which is best suited for all the. . For this, we can use a method called non-maxim suppression (NMS) which basically cleans up after these detections. We can define a certain threshold that would act as a constraint for this NMS method where it would ignore all the other bounding boxes whose confidence is below the threshold mentioned, thus eliminating a few. But this wouldn’t eliminate all, so the next step in the NMS would be implemented, i.e to arrange all the confidences of the bounding boxes in descending order and choose the one with the highest score as the most appropriate one for the object. Then we find all the other boxes with high Intersection over union (IOU) with the bounding box with maximum confidence and eliminate all those as well. . References: . A Gentle Introduction to Object Recognition With Deep Learning . Bounding box object detectors: understanding YOLO, You Look Only Once . What’s new in YOLO v3? . Digging deep into YOLO V3 - A hands-on guide . Dive Really Deep into YOLO v3: A Beginner’s Guide . YOLO_v1 . YOLO_v2 . YOLO_v3 . If you need help, there’re plenty of excellent resources like Udacity Computer Vision Nanodegree, Cousera Deep Learning Specialization and Stanford CS231n. . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive .",
            "url": "https://sidpro-hash.github.io/Deep_Learning/2021/05/06/Object-Detection-YOLOv3.html",
            "relUrl": "/2021/05/06/Object-Detection-YOLOv3.html",
            "date": " • May 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How to Train YOLOv5",
            "content": "Object Detection Models . All of the YOLO models are object detection models. Object detection models are trained to look at an image and search for a subset of object classes. When found, these object classes are enclosed in a bounding box and their class is identified. Object detection models are typically trained and evaluated on the COCO dataset which contains a broad range of 80 object classes. From there, it is assumed that object detection models will generalize to new object detection tasks if they are exposed to new training data. Here is an example of me using YOLOv5 to detect helmet. . Install Dependencies . (Remember to choose GPU in Runtime if not already selected. Runtime --&gt; Change Runtime Type --&gt; Hardware accelerator --&gt; GPU) . %cd /content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/ !pip install -qr requirements.txt # install dependencies (ignore errors) import torch from IPython.display import Image, clear_output # to display images from utils.google_utils import gdrive_download # to download models/datasets # clear_output() print(&#39;Setup complete. Using torch %s %s&#39; % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else &#39;CPU&#39;)) . /content/drive/MyDrive/Helment_Detection_Yolov5/yolov5 |████████████████████████████████| 645kB 22.4MB/s Setup complete. Using torch 1.8.1+cu101 _CudaDeviceProperties(name=&#39;Tesla T4&#39;, major=7, minor=5, total_memory=15109MB, multi_processor_count=40) . Mount Google Drive . We&#39;ll download our dataset from Roboflow. Use the &quot;YOLOv5 PyTorch&quot; export format. Note that the Ultralytics implementation calls for a YAML file defining where your training and test data is. The Roboflow export also writes this format for us. . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . %cd /content/drive/MyDrive/Helment_Detection_Yolov5/ %cat data.yaml . /content/drive/MyDrive/Helment_Detection_Yolov5 train: ../train/images val: ../valid/images nc: 2 names: [&#39;With Helmet&#39;, &#39;Without Helmet&#39;] . Define Model Configuration and Architecture . We will write a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer. . You do not need to edit these cells, but you may. . import yaml with open(&quot;/content/drive/MyDrive/Helment_Detection_Yolov5/data.yaml&quot;, &#39;r&#39;) as stream: num_classes = str(yaml.safe_load(stream)[&#39;nc&#39;]) . . %cat /content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/models/yolov5l.yaml . # parameters nc: 80 # number of classes depth_multiple: 1.0 # model depth multiple width_multiple: 1.0 # layer channel multiple # anchors anchors: - [10,13, 16,30, 33,23] # P3/8 - [30,61, 62,45, 59,119] # P4/16 - [116,90, 156,198, 373,326] # P5/32 # YOLOv5 backbone backbone: # [from, number, module, args] [[-1, 1, Focus, [64, 3]], # 0-P1/2 [-1, 1, Conv, [128, 3, 2]], # 1-P2/4 [-1, 3, C3, [128]], [-1, 1, Conv, [256, 3, 2]], # 3-P3/8 [-1, 9, C3, [256]], [-1, 1, Conv, [512, 3, 2]], # 5-P4/16 [-1, 9, C3, [512]], [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32 [-1, 1, SPP, [1024, [5, 9, 13]]], [-1, 3, C3, [1024, False]], # 9 ] # YOLOv5 head head: [[-1, 1, Conv, [512, 1, 1]], [-1, 1, nn.Upsample, [None, 2, &#39;nearest&#39;]], [[-1, 6], 1, Concat, [1]], # cat backbone P4 [-1, 3, C3, [512, False]], # 13 [-1, 1, Conv, [256, 1, 1]], [-1, 1, nn.Upsample, [None, 2, &#39;nearest&#39;]], [[-1, 4], 1, Concat, [1]], # cat backbone P3 [-1, 3, C3, [256, False]], # 17 (P3/8-small) [-1, 1, Conv, [256, 3, 2]], [[-1, 14], 1, Concat, [1]], # cat head P4 [-1, 3, C3, [512, False]], # 20 (P4/16-medium) [-1, 1, Conv, [512, 3, 2]], [[-1, 10], 1, Concat, [1]], # cat head P5 [-1, 3, C3, [1024, False]], # 23 (P5/32-large) [[17, 20, 23], 1, Detect, [nc, anchors]], # Detect(P3, P4, P5) ] . The Anatomy of an Object Detector . . All object detectors take an image in for input and compress features down through a convolutional neural network backbone. In image classification, these backbones are the end of the network and prediction can be made off of them. . In object detection, multiple bounding boxes need to be drawn around images along with classification, so the feature layers of the convolutional backbone need to be mixed and held up in light of one another. The combination of backbone feature layers happens in the neck. . It is also useful to split object detectors into two categories: one-stage detectors and two stage detectors. Detection happens in the head. . Two-stage detectors decouple the task of object localization and classification for each bounding box. . One-stage detectors make the predictions for object localization and classification at the same time. YOLO is a one-stage detector, hence, You Only Look Once.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from IPython.core.magic import register_line_cell_magic @register_line_cell_magic def writetemplate(line, cell): with open(line, &#39;w&#39;) as f: f.write(cell.format(**globals())) . %%writetemplate /content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/models/custom_yolov5l.yaml # parameters nc: {num_classes} # number of classes depth_multiple: 1.0 # model depth multiple width_multiple: 1.0 # layer channel multiple # anchors anchors: - [10,13, 16,30, 33,23] # P3/8 - [30,61, 62,45, 59,119] # P4/16 - [116,90, 156,198, 373,326] # P5/32 # YOLOv5 backbone backbone: # [from, number, module, args] [[-1, 1, Focus, [64, 3]], # 0-P1/2 [-1, 1, Conv, [128, 3, 2]], # 1-P2/4 [-1, 3, C3, [128]], [-1, 1, Conv, [256, 3, 2]], # 3-P3/8 [-1, 9, C3, [256]], [-1, 1, Conv, [512, 3, 2]], # 5-P4/16 [-1, 9, C3, [512]], [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32 [-1, 1, SPP, [1024, [5, 9, 13]]], [-1, 3, C3, [1024, False]], # 9 ] # YOLOv5 head head: [[-1, 1, Conv, [512, 1, 1]], [-1, 1, nn.Upsample, [None, 2, &#39;nearest&#39;]], [[-1, 6], 1, Concat, [1]], # cat backbone P4 [-1, 3, C3, [512, False]], # 13 [-1, 1, Conv, [256, 1, 1]], [-1, 1, nn.Upsample, [None, 2, &#39;nearest&#39;]], [[-1, 4], 1, Concat, [1]], # cat backbone P3 [-1, 3, C3, [256, False]], # 17 (P3/8-small) [-1, 1, Conv, [256, 3, 2]], [[-1, 14], 1, Concat, [1]], # cat head P4 [-1, 3, C3, [512, False]], # 20 (P4/16-medium) [-1, 1, Conv, [512, 3, 2]], [[-1, 10], 1, Concat, [1]], # cat head P5 [-1, 3, C3, [1024, False]], # 23 (P5/32-large) [[17, 20, 23], 1, Detect, [nc, anchors]], # Detect(P3, P4, P5) ] . The Detection Step . Train Helment Detector YOLOv5 . Here, we are able to pass a number of arguments: . img: define input image size | batch: determine batch size | epochs: define the number of training epochs. (Note: often, 3000+ are common here!) | data: set the path to our yaml file | cfg: specify our model configuration | weights: specify a custom path to weights. (Note: you can download weights from the Ultralytics Google Drive folder) | name: result names | nosave: only save the final checkpoint | cache: cache images for faster training | . # time its performance %%time %cd /content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/ !python train.py --img 416 --batch 16 --epochs 100 --data &#39;../data.yaml&#39; --cfg ./models/custom_yolov5l.yaml --weights &#39;&#39; --name yolov5s_results --cache . /content/drive/MyDrive/Helment_Detection_Yolov5/yolov5 remote: Enumerating objects: 61, done. remote: Counting objects: 100% (57/57), done. remote: Compressing objects: 100% (32/32), done. remote: Total 61 (delta 31), reused 37 (delta 25), pack-reused 4 Unpacking objects: 100% (61/61), done. From https://github.com/ultralytics/yolov5 1849916..955eea8 master -&gt; origin/master c9c95fb..0824388 study_activations -&gt; origin/study_activations github: ⚠️ WARNING: code is out of date by 118 commits. Use &#39;git pull&#39; to update or &#39;git clone https://github.com/ultralytics/yolov5&#39; to download latest. YOLOv5 v4.0-126-g886f1c0 torch 1.8.1+cu101 CUDA:0 (Tesla K80, 11441.1875MB) Namespace(adam=False, batch_size=16, bucket=&#39;&#39;, cache_images=True, cfg=&#39;./models/custom_yolov5l.yaml&#39;, data=&#39;../data.yaml&#39;, device=&#39;&#39;, entity=None, epochs=100, evolve=False, exist_ok=False, global_rank=-1, hyp=&#39;data/hyp.scratch.yaml&#39;, image_weights=False, img_size=[416, 416], linear_lr=False, local_rank=-1, log_artifacts=False, log_imgs=16, multi_scale=False, name=&#39;yolov5s_results&#39;, noautoanchor=False, nosave=False, notest=False, project=&#39;runs/train&#39;, quad=False, rect=False, resume=False, save_dir=&#39;runs/train/yolov5s_results2&#39;, single_cls=False, sync_bn=False, total_batch_size=16, weights=&#39;&#39;, workers=8, world_size=1) wandb: Install Weights &amp; Biases for YOLOv5 logging with &#39;pip install wandb&#39; (recommended) Start Tensorboard with &#34;tensorboard --logdir runs/train&#34;, view at http://localhost:6006/ 2021-04-30 04:32:56.737730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 hyperparameters: lr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0 from n params module arguments 0 -1 1 7040 models.common.Focus [3, 64, 3] 1 -1 1 73984 models.common.Conv [64, 128, 3, 2] 2 -1 1 156928 models.common.C3 [128, 128, 3] 3 -1 1 295424 models.common.Conv [128, 256, 3, 2] 4 -1 1 1611264 models.common.C3 [256, 256, 9] 5 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 6 -1 1 6433792 models.common.C3 [512, 512, 9] 7 -1 1 4720640 models.common.Conv [512, 1024, 3, 2] 8 -1 1 2624512 models.common.SPP [1024, 1024, [5, 9, 13]] 9 -1 1 9971712 models.common.C3 [1024, 1024, 3, False] 10 -1 1 525312 models.common.Conv [1024, 512, 1, 1] 11 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, &#39;nearest&#39;] 12 [-1, 6] 1 0 models.common.Concat [1] 13 -1 1 2757632 models.common.C3 [1024, 512, 3, False] 14 -1 1 131584 models.common.Conv [512, 256, 1, 1] 15 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, &#39;nearest&#39;] 16 [-1, 4] 1 0 models.common.Concat [1] 17 -1 1 690688 models.common.C3 [512, 256, 3, False] 18 -1 1 590336 models.common.Conv [256, 256, 3, 2] 19 [-1, 14] 1 0 models.common.Concat [1] 20 -1 1 2495488 models.common.C3 [512, 512, 3, False] 21 -1 1 2360320 models.common.Conv [512, 512, 3, 2] 22 [-1, 10] 1 0 models.common.Concat [1] 23 -1 1 9971712 models.common.C3 [1024, 1024, 3, False] 24 [17, 20, 23] 1 37695 models.yolo.Detect [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]] Model Summary: 499 layers, 46636735 parameters, 46636735 gradients, 114.3 GFLOPS Scaled weight_decay = 0.0005 Optimizer groups: 110 .bias, 110 conv.weight, 107 other train: Scanning &#39;../train/labels&#39; for images and labels... 1320 found, 0 missing, 24 empty, 0 corrupted: 100% 1320/1320 [00:03&lt;00:00, 376.62it/s] train: New cache created: ../train/labels.cache train: Caching images (0.7GB): 100% 1320/1320 [00:05&lt;00:00, 261.41it/s] val: Scanning &#39;../valid/labels&#39; for images and labels... 126 found, 0 missing, 0 empty, 0 corrupted: 100% 126/126 [00:00&lt;00:00, 252.30it/s] val: New cache created: ../valid/labels.cache val: Caching images (0.1GB): 100% 126/126 [00:00&lt;00:00, 230.62it/s] Plotting labels... autoanchor: Analyzing anchors... anchors/target = 6.19, Best Possible Recall (BPR) = 1.0000 Image sizes 416 train, 416 test Using 2 dataloader workers Logging results to runs/train/yolov5s_results2 Starting training for 100 epochs... Epoch gpu_mem box obj cls total targets img_size 0/99 4.26G 0.09697 0.02842 0.02643 0.1518 32 416: 100% 83/83 [02:15&lt;00:00, 1.64s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:14&lt;00:00, 3.61s/it] all 126 240 0.00177 0.279 0.000776 0.000123 Epoch gpu_mem box obj cls total targets img_size 1/99 4.44G 0.09161 0.02987 0.02503 0.1465 20 416: 100% 83/83 [02:00&lt;00:00, 1.45s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.12s/it] all 126 240 0.00259 0.17 0.000594 8.9e-05 Epoch gpu_mem box obj cls total targets img_size 2/99 4.44G 0.08948 0.03019 0.02386 0.1435 34 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.12s/it] all 126 240 0.00136 0.213 0.000446 6.6e-05 Epoch gpu_mem box obj cls total targets img_size 3/99 4.44G 0.0894 0.03097 0.0234 0.1438 25 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.12s/it] all 126 240 0.0014 0.215 0.000488 7.06e-05 Epoch gpu_mem box obj cls total targets img_size 4/99 4.44G 0.0893 0.02934 0.0238 0.1424 17 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.12s/it] all 126 240 0.00785 0.00303 0.00046 6.84e-05 Epoch gpu_mem box obj cls total targets img_size 5/99 4.44G 0.08878 0.03031 0.02349 0.1426 29 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.12s/it] all 126 240 0.0085 0.0182 0.00104 0.000164 Epoch gpu_mem box obj cls total targets img_size 6/99 4.44G 0.08824 0.03058 0.02332 0.1421 28 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.12s/it] all 126 240 0.0161 0.00909 0.00139 0.000307 Epoch gpu_mem box obj cls total targets img_size 7/99 4.44G 0.08661 0.03147 0.02371 0.1418 19 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.13s/it] all 126 240 0.0112 0.0121 0.00213 0.000361 Epoch gpu_mem box obj cls total targets img_size 8/99 4.44G 0.08436 0.03261 0.02315 0.1401 18 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:05&lt;00:00, 1.26s/it] all 126 240 0.0219 0.0424 0.0066 0.00141 Epoch gpu_mem box obj cls total targets img_size 9/99 4.44G 0.07807 0.0301 0.02296 0.1311 16 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:07&lt;00:00, 1.83s/it] all 126 240 0.0988 0.101 0.0342 0.00713 Epoch gpu_mem box obj cls total targets img_size 10/99 4.44G 0.07444 0.02741 0.02231 0.1242 25 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:05&lt;00:00, 1.27s/it] all 126 240 0.102 0.118 0.0554 0.0134 Epoch gpu_mem box obj cls total targets img_size 11/99 4.44G 0.06966 0.02835 0.02108 0.1191 23 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.13s/it] all 126 240 0.167 0.197 0.127 0.028 Epoch gpu_mem box obj cls total targets img_size 12/99 4.44G 0.06595 0.02687 0.01959 0.1124 15 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.17s/it] all 126 240 0.137 0.258 0.124 0.0365 Epoch gpu_mem box obj cls total targets img_size 13/99 4.44G 0.06328 0.02514 0.01922 0.1076 32 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:06&lt;00:00, 1.53s/it] all 126 240 0.227 0.213 0.15 0.0547 Epoch gpu_mem box obj cls total targets img_size 14/99 4.44G 0.05947 0.02458 0.01807 0.1021 17 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.10s/it] all 126 240 0.186 0.508 0.235 0.0707 Epoch gpu_mem box obj cls total targets img_size 15/99 4.44G 0.05721 0.02384 0.01636 0.09741 30 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.06s/it] all 126 240 0.461 0.316 0.309 0.114 Epoch gpu_mem box obj cls total targets img_size 16/99 4.44G 0.05509 0.02422 0.01478 0.09408 16 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.06s/it] all 126 240 0.411 0.413 0.31 0.0948 Epoch gpu_mem box obj cls total targets img_size 17/99 4.44G 0.05315 0.02356 0.01371 0.09042 41 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.06s/it] all 126 240 0.404 0.428 0.343 0.135 Epoch gpu_mem box obj cls total targets img_size 18/99 4.44G 0.05281 0.02244 0.01317 0.08842 23 416: 100% 83/83 [01:59&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.07s/it] all 126 240 0.363 0.349 0.289 0.107 Epoch gpu_mem box obj cls total targets img_size 19/99 4.44G 0.05009 0.02225 0.01232 0.08465 25 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.05s/it] all 126 240 0.526 0.431 0.398 0.116 Epoch gpu_mem box obj cls total targets img_size 20/99 4.44G 0.04905 0.02106 0.01291 0.08302 19 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.05s/it] all 126 240 0.538 0.516 0.43 0.176 Epoch gpu_mem box obj cls total targets img_size 21/99 4.44G 0.04894 0.02148 0.01124 0.08165 20 416: 100% 83/83 [01:59&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.04s/it] all 126 240 0.527 0.462 0.447 0.166 Epoch gpu_mem box obj cls total targets img_size 22/99 4.44G 0.04723 0.02099 0.01069 0.07891 21 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.562 0.5 0.473 0.196 Epoch gpu_mem box obj cls total targets img_size 23/99 4.44G 0.04767 0.02068 0.01131 0.07966 36 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.14s/it] all 126 240 0.00955 0.178 0.0011 0.000278 Epoch gpu_mem box obj cls total targets img_size 24/99 4.44G 0.04795 0.02099 0.01071 0.07964 35 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.437 0.518 0.421 0.141 Epoch gpu_mem box obj cls total targets img_size 25/99 4.44G 0.04608 0.02114 0.009783 0.077 21 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.569 0.551 0.501 0.209 Epoch gpu_mem box obj cls total targets img_size 26/99 4.44G 0.04454 0.02026 0.008806 0.07361 13 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.433 0.546 0.447 0.171 Epoch gpu_mem box obj cls total targets img_size 27/99 4.44G 0.04534 0.02042 0.009341 0.0751 20 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.04s/it] all 126 240 0.656 0.517 0.505 0.2 Epoch gpu_mem box obj cls total targets img_size 28/99 4.44G 0.04466 0.01946 0.008201 0.07232 19 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.04s/it] all 126 240 0.584 0.575 0.516 0.21 Epoch gpu_mem box obj cls total targets img_size 29/99 4.44G 0.04371 0.01889 0.007327 0.06993 39 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.585 0.616 0.583 0.23 Epoch gpu_mem box obj cls total targets img_size 30/99 4.44G 0.04287 0.0196 0.00685 0.06933 13 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.527 0.598 0.512 0.218 Epoch gpu_mem box obj cls total targets img_size 31/99 4.44G 0.04187 0.01821 0.007428 0.06751 22 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.595 0.566 0.532 0.237 Epoch gpu_mem box obj cls total targets img_size 32/99 4.44G 0.04209 0.01821 0.007386 0.06768 21 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.471 0.523 0.371 0.138 Epoch gpu_mem box obj cls total targets img_size 33/99 4.44G 0.04089 0.0181 0.006372 0.06536 16 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.04s/it] all 126 240 0.661 0.521 0.536 0.228 Epoch gpu_mem box obj cls total targets img_size 34/99 4.44G 0.04211 0.01914 0.006914 0.06816 19 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.539 0.539 0.487 0.194 Epoch gpu_mem box obj cls total targets img_size 35/99 4.44G 0.04096 0.01867 0.006569 0.0662 19 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.644 0.504 0.509 0.208 Epoch gpu_mem box obj cls total targets img_size 36/99 4.44G 0.041 0.01861 0.006305 0.06591 18 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.554 0.562 0.517 0.223 Epoch gpu_mem box obj cls total targets img_size 37/99 4.44G 0.0409 0.01825 0.005963 0.06511 24 416: 100% 83/83 [01:58&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.566 0.57 0.536 0.227 Epoch gpu_mem box obj cls total targets img_size 38/99 4.44G 0.03975 0.01876 0.006003 0.06451 10 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.614 0.59 0.555 0.231 Epoch gpu_mem box obj cls total targets img_size 39/99 4.44G 0.03928 0.0182 0.00552 0.063 37 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.686 0.53 0.57 0.221 Epoch gpu_mem box obj cls total targets img_size 40/99 4.44G 0.03948 0.01697 0.005399 0.06185 20 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.604 0.55 0.536 0.209 Epoch gpu_mem box obj cls total targets img_size 41/99 4.44G 0.03818 0.01732 0.004386 0.05988 26 416: 100% 83/83 [01:58&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.572 0.63 0.575 0.236 Epoch gpu_mem box obj cls total targets img_size 42/99 4.44G 0.03826 0.01746 0.005158 0.06088 15 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.54 0.618 0.557 0.24 Epoch gpu_mem box obj cls total targets img_size 43/99 4.44G 0.03741 0.01685 0.004884 0.05914 25 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.548 0.557 0.519 0.229 Epoch gpu_mem box obj cls total targets img_size 44/99 4.44G 0.03721 0.0169 0.004807 0.05891 21 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.597 0.573 0.58 0.268 Epoch gpu_mem box obj cls total targets img_size 45/99 4.44G 0.03686 0.01648 0.004751 0.05809 27 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.63 0.607 0.616 0.264 Epoch gpu_mem box obj cls total targets img_size 46/99 4.44G 0.0373 0.01707 0.004799 0.05918 12 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.01s/it] all 126 240 0.617 0.609 0.578 0.25 Epoch gpu_mem box obj cls total targets img_size 47/99 4.44G 0.0358 0.01641 0.003825 0.05604 35 416: 100% 83/83 [01:58&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.536 0.614 0.526 0.218 Epoch gpu_mem box obj cls total targets img_size 48/99 4.44G 0.03578 0.01641 0.003722 0.05591 33 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.645 0.544 0.571 0.25 Epoch gpu_mem box obj cls total targets img_size 49/99 4.44G 0.03572 0.01627 0.004139 0.05613 23 416: 100% 83/83 [01:58&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.539 0.622 0.569 0.255 Epoch gpu_mem box obj cls total targets img_size 50/99 4.44G 0.0346 0.0162 0.003541 0.05434 26 416: 100% 83/83 [01:58&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.568 0.625 0.581 0.247 Epoch gpu_mem box obj cls total targets img_size 51/99 4.44G 0.03559 0.01588 0.003708 0.05518 14 416: 100% 83/83 [01:58&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.614 0.602 0.578 0.229 Epoch gpu_mem box obj cls total targets img_size 52/99 4.44G 0.03397 0.01529 0.003692 0.05295 10 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.673 0.61 0.616 0.281 Epoch gpu_mem box obj cls total targets img_size 53/99 4.44G 0.03333 0.01531 0.003498 0.05214 16 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.638 0.606 0.611 0.276 Epoch gpu_mem box obj cls total targets img_size 54/99 4.44G 0.03375 0.01557 0.003505 0.05283 20 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.674 0.61 0.624 0.291 Epoch gpu_mem box obj cls total targets img_size 55/99 4.44G 0.03376 0.01552 0.003477 0.05276 28 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.608 0.587 0.555 0.24 Epoch gpu_mem box obj cls total targets img_size 56/99 4.44G 0.03314 0.0145 0.003712 0.05134 18 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.04s/it] all 126 240 0.593 0.627 0.59 0.246 Epoch gpu_mem box obj cls total targets img_size 57/99 4.44G 0.03285 0.0153 0.003178 0.05133 28 416: 100% 83/83 [01:58&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.616 0.603 0.598 0.266 Epoch gpu_mem box obj cls total targets img_size 58/99 4.44G 0.03249 0.01498 0.003033 0.0505 18 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.62 0.62 0.593 0.264 Epoch gpu_mem box obj cls total targets img_size 59/99 4.44G 0.0318 0.01429 0.002914 0.049 21 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.577 0.656 0.571 0.251 Epoch gpu_mem box obj cls total targets img_size 60/99 4.44G 0.03215 0.01468 0.002896 0.04973 16 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.57 0.641 0.565 0.262 Epoch gpu_mem box obj cls total targets img_size 61/99 4.44G 0.03266 0.01485 0.002792 0.05031 27 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.567 0.635 0.58 0.244 Epoch gpu_mem box obj cls total targets img_size 62/99 4.44G 0.0315 0.01464 0.002327 0.04846 22 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.589 0.573 0.57 0.259 Epoch gpu_mem box obj cls total targets img_size 63/99 4.44G 0.03142 0.01457 0.002556 0.04855 26 416: 100% 83/83 [01:59&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.718 0.536 0.597 0.275 Epoch gpu_mem box obj cls total targets img_size 64/99 4.44G 0.03094 0.01422 0.002988 0.04815 23 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.649 0.651 0.622 0.267 Epoch gpu_mem box obj cls total targets img_size 65/99 4.44G 0.03088 0.01405 0.002781 0.04771 24 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.611 0.634 0.599 0.258 Epoch gpu_mem box obj cls total targets img_size 66/99 4.44G 0.03033 0.01412 0.002732 0.04718 21 416: 100% 83/83 [01:59&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.709 0.549 0.565 0.246 Epoch gpu_mem box obj cls total targets img_size 67/99 4.44G 0.03021 0.01438 0.002187 0.04677 22 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.619 0.596 0.558 0.228 Epoch gpu_mem box obj cls total targets img_size 68/99 4.44G 0.02937 0.01381 0.002263 0.04545 23 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.588 0.639 0.592 0.262 Epoch gpu_mem box obj cls total targets img_size 69/99 4.44G 0.02973 0.01403 0.00233 0.04609 30 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.68 0.611 0.601 0.268 Epoch gpu_mem box obj cls total targets img_size 70/99 4.44G 0.02883 0.01385 0.002633 0.04531 17 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.658 0.632 0.597 0.268 Epoch gpu_mem box obj cls total targets img_size 71/99 4.44G 0.02941 0.0133 0.002229 0.04495 12 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.721 0.555 0.601 0.266 Epoch gpu_mem box obj cls total targets img_size 72/99 4.44G 0.02934 0.01371 0.00248 0.04553 26 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.632 0.587 0.58 0.246 Epoch gpu_mem box obj cls total targets img_size 73/99 4.44G 0.02899 0.01349 0.002007 0.04448 15 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.646 0.568 0.59 0.261 Epoch gpu_mem box obj cls total targets img_size 74/99 4.44G 0.02832 0.01341 0.002194 0.04392 20 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.594 0.612 0.581 0.264 Epoch gpu_mem box obj cls total targets img_size 75/99 4.44G 0.0285 0.0134 0.002231 0.04413 23 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.619 0.622 0.588 0.255 Epoch gpu_mem box obj cls total targets img_size 76/99 4.44G 0.02858 0.0137 0.002069 0.04435 13 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.645 0.635 0.611 0.258 Epoch gpu_mem box obj cls total targets img_size 77/99 4.44G 0.02732 0.01295 0.002323 0.04259 21 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.623 0.602 0.58 0.252 Epoch gpu_mem box obj cls total targets img_size 78/99 4.44G 0.02745 0.01272 0.001845 0.04202 21 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.59 0.648 0.58 0.256 Epoch gpu_mem box obj cls total targets img_size 79/99 4.44G 0.02756 0.01276 0.002626 0.04294 22 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.604 0.621 0.609 0.266 Epoch gpu_mem box obj cls total targets img_size 80/99 4.44G 0.02747 0.01294 0.002191 0.0426 16 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.646 0.615 0.6 0.259 Epoch gpu_mem box obj cls total targets img_size 81/99 4.44G 0.02718 0.01288 0.002069 0.04213 17 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.645 0.636 0.61 0.256 Epoch gpu_mem box obj cls total targets img_size 82/99 4.44G 0.02662 0.01251 0.002086 0.04122 19 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.622 0.644 0.593 0.252 Epoch gpu_mem box obj cls total targets img_size 83/99 4.44G 0.02651 0.01305 0.002263 0.04182 27 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.743 0.566 0.607 0.284 Epoch gpu_mem box obj cls total targets img_size 84/99 4.44G 0.02664 0.0123 0.002308 0.04125 20 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.693 0.599 0.591 0.268 Epoch gpu_mem box obj cls total targets img_size 85/99 4.44G 0.02639 0.01309 0.001987 0.04147 30 416: 100% 83/83 [01:59&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.694 0.581 0.604 0.27 Epoch gpu_mem box obj cls total targets img_size 86/99 4.44G 0.02645 0.01254 0.002108 0.0411 17 416: 100% 83/83 [01:59&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.733 0.561 0.607 0.275 Epoch gpu_mem box obj cls total targets img_size 87/99 4.44G 0.02632 0.01244 0.001923 0.04068 21 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.04s/it] all 126 240 0.622 0.615 0.587 0.266 Epoch gpu_mem box obj cls total targets img_size 88/99 4.44G 0.0259 0.01239 0.001953 0.04024 25 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.687 0.558 0.584 0.268 Epoch gpu_mem box obj cls total targets img_size 89/99 4.44G 0.02561 0.01237 0.001465 0.03944 26 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.666 0.583 0.595 0.258 Epoch gpu_mem box obj cls total targets img_size 90/99 4.44G 0.02565 0.01154 0.001819 0.03901 23 416: 100% 83/83 [01:59&lt;00:00, 1.44s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.636 0.586 0.573 0.264 Epoch gpu_mem box obj cls total targets img_size 91/99 4.44G 0.02573 0.01259 0.002049 0.04037 33 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.598 0.654 0.6 0.275 Epoch gpu_mem box obj cls total targets img_size 92/99 4.44G 0.02565 0.0118 0.001828 0.03929 18 416: 100% 83/83 [01:57&lt;00:00, 1.41s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.668 0.572 0.58 0.254 Epoch gpu_mem box obj cls total targets img_size 93/99 4.44G 0.02547 0.01187 0.001887 0.03923 19 416: 100% 83/83 [01:58&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.601 0.615 0.588 0.253 Epoch gpu_mem box obj cls total targets img_size 94/99 4.44G 0.02589 0.01213 0.001698 0.03972 20 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.672 0.595 0.596 0.285 Epoch gpu_mem box obj cls total targets img_size 95/99 4.44G 0.025 0.01178 0.001609 0.03839 24 416: 100% 83/83 [01:57&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.02s/it] all 126 240 0.658 0.579 0.595 0.266 Epoch gpu_mem box obj cls total targets img_size 96/99 4.44G 0.02547 0.01245 0.001805 0.03972 16 416: 100% 83/83 [01:58&lt;00:00, 1.42s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.04s/it] all 126 240 0.637 0.626 0.61 0.272 Epoch gpu_mem box obj cls total targets img_size 97/99 4.44G 0.0247 0.01193 0.001771 0.0384 25 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.64 0.578 0.591 0.27 Epoch gpu_mem box obj cls total targets img_size 98/99 4.44G 0.02499 0.01186 0.00169 0.03854 43 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:04&lt;00:00, 1.03s/it] all 126 240 0.688 0.545 0.587 0.266 Epoch gpu_mem box obj cls total targets img_size 99/99 4.44G 0.02491 0.0112 0.001944 0.03805 17 416: 100% 83/83 [01:58&lt;00:00, 1.43s/it] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 4/4 [00:05&lt;00:00, 1.27s/it] all 126 240 0.594 0.637 0.593 0.264 With Helmet 126 165 0.645 0.782 0.696 0.31 Without Helmet 126 75 0.543 0.493 0.491 0.219 Optimizer stripped from runs/train/yolov5s_results2/weights/last.pt, 93.7MB Optimizer stripped from runs/train/yolov5s_results2/weights/best.pt, 93.7MB 100 epochs completed in 3.499 hours. CPU times: user 1min 1s, sys: 8.35 s, total: 1min 10s Wall time: 3h 30min 55s . Evaluate Custom YOLOv5 Detector Performance . Training losses and performance metrics are saved to Tensorboard and also to a logfile defined above with the --name flag when we train. In our case, we named this yolov5s_results. (If given no name, it defaults to results.txt.) The results file is plotted as a png after training completes. . Note from Glenn: Partially completed results.txt files can be plotted with from utils.utils import plot_results; plot_results(). . # Launch after you have started training # logs save in the folder &quot;runs&quot; #%load_ext tensorboard #%tensorboard --logdir runs . from utils.plots import plot_results # plot results.txt as results.png Image(filename=&#39;/content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/runs/train/yolov5s_results2/results.png&#39;, width=1000) # view results.png #Image(filename=&#39;/content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/runs/train/yolov5s_results/results.png&#39;, width=1000) # view results.png . Curious? Visualize Our Training Data with Labels . After training starts, view train*.jpg images to see training images, labels and augmentation effects. . Note a mosaic dataloader is used for training (shown below), a new dataloading concept developed by Glenn Jocher and first featured in YOLOv4. . print(&quot;GROUND TRUTH TRAINING DATA:&quot;) Image(filename=&#39;/content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/runs/train/yolov5s_results2/test_batch0_labels.jpg&#39;, width=900) . GROUND TRUTH TRAINING DATA: . print(&quot;GROUND TRUTH AUGMENTED TRAINING DATA:&quot;) Image(filename=&#39;/content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/runs/train/yolov5s_results2/train_batch0.jpg&#39;, width=900) . GROUND TRUTH AUGMENTED TRAINING DATA: . Run With Trained Weights . Run inference with a pretrained checkpoint on contents of test/images folder downloaded from Roboflow. . %cd /content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/ %ls runs/ . /content/drive/MyDrive/Helment_Detection_Yolov5/yolov5 detect/ train/ . %ls runs/train/yolov5s_results2/weights . best.pt last.pt . # use the best weights! %cd /content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/ !python detect.py --weights runs/train/yolov5s_results2/weights/last.pt --img 416 --conf 0.1 --source ../traffic #!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.4 --source 0 #!python detect.py --weights runs/train/yolov5s_results2/weights/best.pt --conf 0.1 --source ../race/Men1.mp4 . /content/drive/MyDrive/Helment_Detection_Yolov5/yolov5 Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.1, device=&#39;&#39;, exist_ok=False, img_size=416, iou_thres=0.45, name=&#39;exp&#39;, project=&#39;runs/detect&#39;, save_conf=False, save_txt=False, source=&#39;../traffic&#39;, update=False, view_img=False, weights=[&#39;runs/train/yolov5s_results2/weights/last.pt&#39;]) YOLOv5 v4.0-126-g886f1c0 torch 1.8.1+cu101 CUDA:0 (Tesla T4, 15109.75MB) Fusing layers... Model Summary: 392 layers, 46605951 parameters, 0 gradients, 114.1 GFLOPS image 1/5 /content/drive/My Drive/Helment_Detection_Yolov5/yolov5/../traffic/traffic-helment13.jpg: 256x416 3 Without Helmets, Done. (0.022s) image 2/5 /content/drive/My Drive/Helment_Detection_Yolov5/yolov5/../traffic/traffic-helment14.jpg: 256x416 8 With Helmets, 1 Without Helmet, Done. (0.021s) image 3/5 /content/drive/My Drive/Helment_Detection_Yolov5/yolov5/../traffic/traffic-helment15.jpg: 288x416 3 With Helmets, 5 Without Helmets, Done. (0.021s) image 4/5 /content/drive/My Drive/Helment_Detection_Yolov5/yolov5/../traffic/traffic-helment31.jpg: 288x416 5 With Helmets, 5 Without Helmets, Done. (0.020s) image 5/5 /content/drive/My Drive/Helment_Detection_Yolov5/yolov5/../traffic/traffic-helment33.jpg: 288x416 2 With Helmets, Done. (0.020s) Results saved to runs/detect/exp11 Done. (1.829s) . #this looks much better with longer training above import glob from IPython.display import Image, display for imageName in glob.glob(&#39;/content/drive/MyDrive/Helment_Detection_Yolov5/yolov5/runs/detect/exp11/*.jpg&#39;): #assuming JPG display(Image(filename=imageName)) print(&quot; n&quot;) . Output hidden; open in https://colab.research.google.com to view. . Export Trained Weights for Future . Now that you have trained your custom detector, you can export the trained weights you have made here for inference on your device elsewhere . . References . Hope you enjoyed this! . How to Train A Custom Object Detection Model with YOLO v5 | LabelImg is a graphical image annotation tool and label object bounding boxes in images | How to Label Images for Object Detection with CVAT | How to Train YOLO v5 on a Custom Dataset | Kaggle Dataset | When Should I Auto-Orient My Images? | YOLOv5 is Here: State-of-the-Art Object Detection at 140 FPS | To train YOLOv5, just drop in your dataset link from Roboflow. | To get your data into Roboflow, follow the Getting Started Guide | Breaking Down YOLOv4 | &lt;/div&gt; .",
            "url": "https://sidpro-hash.github.io/Deep_Learning/2021/05/06/Helment-Detection-YOLOv5.html",
            "relUrl": "/2021/05/06/Helment-Detection-YOLOv5.html",
            "date": " • May 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://sidpro-hash.github.io/Deep_Learning/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sidpro-hash.github.io/Deep_Learning/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sidpro-hash.github.io/Deep_Learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sidpro-hash.github.io/Deep_Learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}